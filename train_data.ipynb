{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305b298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy as np\n",
    "\n",
    "from net import SignPredictor\n",
    "from activation_functions import der_relu, relu, sigmoid\n",
    "from loss_functions import binary_entropy_loss\n",
    "from optimizer import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fefe5d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the training data set:  348\n",
      "Number of samples in the test data set:  62\n",
      "x train:  (348, 4096)\n",
      "x test:  (62, 4096)\n",
      "y train:  (348, 1)\n",
      "y test:  (62, 1)\n"
     ]
    }
   ],
   "source": [
    "x_l = np.load('../data/data/X.npy')\n",
    "Y_l = np.load('../data/data/Y.npy')\n",
    "# Join the input images that display zeros and ones along the row axis.\n",
    "# Results in X with indices 0 to 204 being zero sign examples and indices 205 to 410 being one sign examples.\n",
    "X = np.concatenate((x_l[204:409], x_l[822:1027] ), axis=0)\n",
    "Y = np.concatenate((np.zeros(205), np.ones(205)), axis=0).reshape(X.shape[0],1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)\n",
    "number_of_train = X_train.shape[0]\n",
    "number_of_test = X_test.shape[0]\n",
    "print(\"Number of samples in the training data set: \" , number_of_train)\n",
    "print(\"Number of samples in the test data set: \" , number_of_test)\n",
    "X_train = X_train.reshape(number_of_train, X_train.shape[1]*X_train.shape[2])\n",
    "X_test = X_test.reshape(number_of_test, X_test.shape[1]*X_test.shape[2])\n",
    "\n",
    "print(\"x train: \", X_train.shape)\n",
    "print(\"x test: \", X_test.shape)\n",
    "print(\"y train: \", Y_train.shape)\n",
    "print(\"y test: \", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4186acdc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[43mbatch_size\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "X_train.shape[0]/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e032f969",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epochs = 2000\n",
    "batch_size = 64\n",
    "batches_per_epoch = np.ceil(X_train.shape[0]/batch_size)\n",
    "optimizer = Optimizer(\n",
    "    lr = 0.03,\n",
    "    adam_args = {\n",
    "        \"beta_1\": 0.9,\n",
    "        \"beta_2\": 0.999,\n",
    "        \"epsilon\": 1e-8,\n",
    "    },\n",
    "    lr_decay_type= \"cosine\",\n",
    "    total_epochs = total_epochs * batches_per_epoch\n",
    ")\n",
    "\n",
    "net = SignPredictor(\n",
    "    input_size = X_train.shape[1],\n",
    "    layer_sizes= [10, 1],\n",
    "    activations = [relu, sigmoid],\n",
    "    activations_der=[der_relu, None],\n",
    "    loss_fn = binary_entropy_loss,\n",
    "    loss_fn_der= None,\n",
    "    optimizer=optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ab31247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.0031, learning rate: 0.02999998149449555\n",
      "Epoch 100, Loss: 0.0012, learning rate: 0.02981162093252581\n",
      "Epoch 200, Loss: 0.0006, learning rate: 0.0292585491062457\n",
      "Epoch 300, Loss: 0.0003, learning rate: 0.028354384479558386\n",
      "Epoch 400, Loss: 0.0002, learning rate: 0.027121390586323266\n",
      "Epoch 500, Loss: 0.0001, learning rate: 0.025589927828263918\n",
      "Epoch 600, Loss: 0.0001, learning rate: 0.023797705901117253\n",
      "Epoch 700, Loss: 0.0000, learning rate: 0.021788855256773182\n",
      "Epoch 800, Loss: 0.0000, learning rate: 0.019612840465088512\n",
      "Epoch 900, Loss: 0.0000, learning rate: 0.017323242232013567\n",
      "Epoch 1000, Loss: 0.0000, learning rate: 0.014976438064787535\n",
      "Epoch 1100, Loss: 0.0000, learning rate: 0.012630214070604018\n",
      "Epoch 1200, Loss: 0.0000, learning rate: 0.010342342070870814\n",
      "Epoch 1300, Loss: 0.0000, learning rate: 0.008169157067233203\n",
      "Epoch 1400, Loss: 0.0000, learning rate: 0.006164170086868255\n",
      "Epoch 1500, Loss: 0.0000, learning rate: 0.004376750563402862\n",
      "Epoch 1600, Loss: 0.0000, learning rate: 0.002850910697610016\n",
      "Epoch 1700, Loss: 0.0000, learning rate: 0.0016242217309577422\n",
      "Epoch 1800, Loss: 0.0000, learning rate: 0.0007268888169522763\n",
      "Epoch 1900, Loss: 0.0000, learning rate: 0.00018100727001363782\n",
      "Final Epoch prediction: [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]], target: [[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Share of correctly predicted images 0.967741935483871\n"
     ]
    }
   ],
   "source": [
    "t = 1\n",
    "for epoch in range(total_epochs):\n",
    "    # Introduce batching:\n",
    "    perm = np.random.permutation(X_train.shape[0])\n",
    "    X_train_shuffled = X_train[perm]\n",
    "    Y_train_shuffled = Y_train[perm]\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i in range(0, X_train.shape[0], batch_size):\n",
    "        X_batch = X_train_shuffled[i:i+batch_size]\n",
    "        Y_batch = Y_train_shuffled[i:i+batch_size]\n",
    "        loss, pred = net.train_step(X_batch, Y_batch, t)\n",
    "        t += 1\n",
    "        epoch_loss += loss * X_batch.shape[0]  # Weight by batch size\n",
    "    \n",
    "    epoch_loss /= X_train.shape[0]\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {epoch_loss:.4f}, learning rate: {optimizer.current_lr}\")\n",
    "\n",
    "print(f\"Final Epoch prediction: {pred.round()}, target: {Y_train}\")\n",
    "\n",
    "test_pred = net.predict(X_test)\n",
    "print(f\"Share of correctly predicted images {np.mean(test_pred.round() == Y_test)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ABMS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
